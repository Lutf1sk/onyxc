import "std/unix.nyx", "std/bitmath.nyx";

mcopy :: void(void* dst, void* src, usz bytes) {
	dst_it : u8* = dst;
	src_it : u8* = src;

	while bytes-- {
		*dst_it++ = *src_it++;
	}
};

mset8 :: void(void* dst, u8 v, usz bytes) {
	it : u8* = dst;
	while bytes-- {
		*it++ = v;
	}
};

vmem_alloc :: void*(usz size) {
	addr := isz:mmap(0, size, PROT.READ|PROT.WRITE|PROT.EXEC, MAP.PRIVATE|MAP.ANONYMOUS, -1, 0);
	if (addr < 0) {
		return null;
	}
	return void*:addr;
};

vmem_free :: void(void* mem, usz size) {
	munmap(usz:mem, size);
};

VMEM_BLOCK_SIZE :: 4096;
ALLOC_DEFAULT_ALIGN :: 16;

ALLOC_FLAGS :: enum usz {
	FIXED_SIZE : 0x01, // Do not grow automatically when out of memory
	DONT_UNMAP : 0x02, // The memory should not be unmapped/deallocated after destruction
};

Allocator :: struct {
	void*(void*, usz) alloc;
	void*(void*, void*, usz) realloc;
	void(void*, void*) free;
	usz(void*, void*) size;
	void* usr;
};

malloc_init :: void[](Allocator* alc, void[] arr) {
	data := alc.alloc(alc.usr, arr.count);
	if !data {
		return null[0..0];
	}
	mcopy(data, arr.data, arr.count);
	return data[0..arr.count];
};

Pool :: struct {
	void* base;
	usz chunk_size;
	usz chunk_count;
	usz size;
	void* head;
	usz flags;
	Allocator* parent;
	Allocator interf;
};

_pmalloc_if :: void*(Pool* pool, usz size) {
	head : void** = pool.head;
	if !head || size > pool.chunk_size {
		return null;
	}
	pool.head = *head;
	return head;
};

_pmrealloc_if :: void*(Pool* pool, void* ptr, usz new_size) {
	if new_size > pool.chunk_size {
		return null;
	}
	return ptr;
};

_pmsize_if :: usz(Pool* pool, void* ptr) {
	return pool.chunk_size;
};

pmalloc :: void*(Pool* pool) {
	head : void** = pool.head;
	if !head {
		return null;
	}
	pool.head = *head;
	return head;
};

pmfree :: void(Pool* pool, void* ptr) {
	*void**:ptr = pool.head;
	pool.head = ptr;
};

pmreset :: void(Pool* pool) {
	last := &pool.head;
	it := pool.base;
	for usz i..pool.chunk_count {
		*last = it;
		last = it;
		it += pool.chunk_size;
	}
	*last = null;
};

pmcreatem :: Pool*(Allocator* parent, void* mem, usz size, usz chunk_size, usz flags) {
	headersz := alignf(sizeof(Pool), ALLOC_DEFAULT_ALIGN);

	if size < headersz || !chunk_size {
		return null;
	}

	if chunk_size < usz:sizeof(usz) {
		chunk_size = sizeof(usz);
	}

	pool : Pool* = mem;
	pool.base = mem + headersz;
	pool.chunk_size = chunk_size;
	pool.size = size;
	pool.chunk_count = (size - headersz) / chunk_size;
	pool.flags = flags;
	pool.parent = parent;
	pool.interf.alloc = void*(void*, usz): _pmalloc_if;
	pool.interf.realloc = void*(void*, void*, usz): _pmrealloc_if;
	pool.interf.free = void(void*, void*): pmfree;
	pool.interf.size = usz(void*, void*): _pmsize_if;
	pool.interf.usr = pool;

	pmreset(pool);
	return pool;
};

pmcreate :: Pool*(Allocator* parent, usz size, usz chunk_size, usz flags) {
	base := null;
	if !parent {
		size = alignf(size, VMEM_BLOCK_SIZE);
		base = vmem_alloc(size);
	}
	else {
		size = alignf(size, ALLOC_DEFAULT_ALIGN);
		base = parent.alloc(parent.usr, size);
	}
	if !base {
		return null;
	}
	return pmcreatem(parent, base, size, chunk_size, flags);
};

pmdestroy :: void(Pool* pool) {
	if (pool.flags & ALLOC_FLAGS.DONT_UNMAP) { return; }

	if pool.parent {
		pool.parent.free(pool.parent.usr, pool);
	}
	else {
		vmem_free(pool.base, pool.size);
	}
};

Arena :: struct {
	void* base;
	usz size;
	void* top;
	usz flags;
	Allocator* parent;
	Allocator interf;
};

amalloc :: void*(Arena* arena, usz size) {
	start := void*:alignf(usz:arena.top, ALLOC_DEFAULT_ALIGN);
	new_top := start + ALLOC_DEFAULT_ALIGN + size;

	if new_top > arena.base + arena.size {
		return null;
	}
	*usz*:start = size;
	arena.top = new_top;
	return start + ALLOC_DEFAULT_ALIGN;
};

amrealloc :: void*(Arena* arena, void* ptr, usz new_size) {
	psize := usz*:(ptr - ALLOC_DEFAULT_ALIGN);

	if arena.top == ptr + *psize {
		if ptr + new_size > arena.base + arena.size {
			return null;
		}
		*psize = new_size;
		arena.top = ptr + new_size;
		return ptr;
	}

	new_ptr := amalloc(arena, new_size);
	if !new_ptr {
		return null;
	}
	mcopy(new_ptr, ptr, *psize);
	return new_ptr;
};

amfree :: void(Arena* arena, void* ptr) {
	old_size := *usz*:(ptr - ALLOC_DEFAULT_ALIGN);
	if arena.top == ptr + old_size {
		arena.top = ptr - ALLOC_DEFAULT_ALIGN;
	}
};

amsize :: usz(Arena* arena, void* ptr) {
	return *usz*:(ptr - ALLOC_DEFAULT_ALIGN);
};

amsave :: void*(Arena* arena) {
	return arena.top;
};

amrestore :: void(Arena* arena, void* old) {
	arena.top = old;
};

amcreatem :: Arena*(Allocator* parent, void* mem, usz size, usz flags) {
	if size < usz:sizeof(Arena) {
		return null;
	}

	arena : Arena* = mem;
	arena.base = mem;
	arena.size = size;
	arena.top = mem + sizeof(Arena); // Reserve enough memory to cover the header
	arena.flags = flags;
	arena.parent = parent;
	arena.interf.alloc = void*(void*, usz): amalloc;
	arena.interf.realloc = void*(void*, void*, usz): amrealloc;
	arena.interf.free = void(void*, void*): amfree;
	arena.interf.size = usz(void*, void*): amsize;
	arena.interf.usr = arena;
	return arena;
};

amcreate :: Arena*(Allocator* parent, usz size, usz flags) {
	base: = null;
	if !parent {
		size = alignf(size, VMEM_BLOCK_SIZE);
		base = vmem_alloc(size);
	}
	else {
		size = alignf(size, ALLOC_DEFAULT_ALIGN);
		base = parent.alloc(parent.usr, size);
	}
	if !base {
		return null;
	}
	return amcreatem(parent, base, size, flags);
};

amdestroy :: void(Arena* arena) {
	if (arena.flags & ALLOC_FLAGS.DONT_UNMAP) { return; }

	if arena.parent {
		arena.parent.free(arena.parent.usr, arena);
	}
	else {
		vmem_free(arena.base, arena.size);
	}
};

